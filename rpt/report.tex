\documentclass{article}

\usepackage[english]{babel}
\usepackage{amsfonts, amsmath, amssymb, MnSymbol, graphicx, hyperref, amsthm, algorithmicx, algpseudocode}
\usepackage[Algorithm,ruled]{algorithm}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% Math Shortcuts
\newcommand{\mbrace}[1]{\ensuremath{\left\{#1\right\}}}
\newcommand{\rpm}{\ensuremath{\raisebox{0.5ex}{$\scriptstyle\pm$}}}
\newcommand{\bfm}{\ensuremath{\mathbf{\mu}}}
\newcommand{\bfs}{\ensuremath{\mathbf{\sigma}}}
\newcommand{\mbf}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\mc}[1]{\ensuremath{\mathcal{#1}}}
\newcommand{\mkpartial}[2]{\ensuremath{\frac{\partial #1}{\partial #2}}}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% algpseudocode helpers
\newcommand{\algtab}{\hspace{\algorithmicindent}}
\newcommand{\algtitle}[1]{\smallskip\hrule\smallskip{\bf #1}\smallskip\hrule}

\newenvironment{algo}[1]
{\noindent\ignorespaces\algtitle{#1}\begin{algorithmic}[1]}
{\end{algorithmic}\hrule\smallskip\ignorespacesafterend}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% Formatting Shortcuts
\newcommand{\result}[1]{\subsubsection*{#1}}
\newcommand{\fakesection}[1]{{\bf #1}\par}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{CAP5638 Project 2\\\smallskip
\large Classification Using Linear Discriminant Functions and Boosting Algorithms}
\author{Suhib Sam Kiswani}
\date{December 2, 2015}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle

The algorithms were implemented in {\it Python 3.5}, with a dependence on the \textit{scipy} \cite{sp} library.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Basic Two-Class Classification Using Perceptron Algorithms}
Abstractly, the problem is as follows:
Given $n$ labeled training samples, $D=\{( x_1, L_1), (x_2, L_2), ..., (x_n, L_n)\}$, where $L_i = \rpm1$, implement Algorithm 4 (Fixed-Increment Single-Sample Perceptron Algorithm) and Algorithm 8 (Batch Relaxation with Margin) of Chapter 5 in the textbook.

\bigskip

The algorithms used for this method are:
\begin{algo}{Algorithm 5.4 (Fixed-Increment Single-Sample Perceptron)}
\State {\bf initialize} $a,k = 0$
\State {\bf do} ~$k \gets (k+1)\mod n$
\State \algtab {\bf if} $\mbf{y}_k$ is misclassified by {\bf a} {\bf then} $\mbf{a} \gets \mbf{a} + \mbf{y}_k$
\State {\bf until} all patterns properly classified
\State {\bf return} $a$
\end{algo}
\bigskip
\begin{algo}{Algorithm 5.8 (Batch Relaxation with Margin)}
\State {\bf initialize} $a,\eta(\cdot),b,k \gets 0$
\State {\bf do} ~$k \gets (k+1)\mod n$
\State\algtab $\mc{Y}_k = \{\}$
\State\algtab $j = 0$
\State\algtab {\bf do} ~$j \gets j + 1$
\State\algtab\algtab {\bf if} $\mbf{a}^t\mbf{y}^j \leq b$ {\bf then} Append $\mbf{y}^j$ to $\mc{Y}_k$
\State\algtab {\bf until} ~$j = n$
\State\algtab $\mbf{a} \gets \mbf{a} + \eta(k)\sum_{y\in\mc{Y}}\frac{b-\mbf{a}^t\mbf{y}}{||\mbf{y}||^2}\mbf{y}$
\State {\bf until} $\mc{Y}_k = \{\}$
\State {\bf return} $a$
\end{algo}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection*{Results}
Long training times proved problematic, due to the fact that it took greater than 100000 iterations to reach convergence using the batch relaxation rule for both data sets. Most likely, this is due to the fact that the mean and standard deviation of the training samples was not normalized (however, the perceptron was trained using normalized augmented features, e.g. for training $\omega_1$, $y^T_i = [1, \mbf{x_i}]$ if $x_i$ belongs to $\omega_1$, and $y^T_i = [-1, -\mbf{x_i}]$ if $x_i$ is not a sample for $\omega_1$.

In order to compensate for the long training times, training would terminate after 100000 trials or if the change in weights was approximately zero and $J_p(a) \approx 0$ (Equation (33) from Chapter 5). It's very likely that these early-termination heuristics had a detrimental effect on the classification accuracy, since in these cases, the there was no guarantee that $\mbf{a}^T\mbf{y} \geq 0$ for all augmented $\mbf{y}$.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\result{UCI Wine Data Set}
\fakesection{Algorithm 5.4 (Fixed-Increment Single-Sample Perceptron)}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline\multicolumn{3}{|c|}{{\bf Training Statistics}}\\
\hline{\bf Class} & {\bf Iterations} & {\bf Runtime (s)}\\\hline
$\omega_1$ & 22864 & 27.498 \\
$\omega_2$ & 52918 & 62.130 \\
$\omega_3$ & 35600 & 127.227\\
\hline
{\bf Total} & 111,382 & 216.855\\
\hline
\end{tabular}
\end{center}

The weights for each class after training were:
\begin{center}
\begin{tabular}{ ccc }
$\mbf{a}_1 = \left[\begin{matrix}
    61035.869 \\ 
    -29616.504 \\ 
    21752.882 \\ 
    1483.633 \\
    -25062.26 \\ 
    157.921 \\ 
    11769.882 \\ 
    39925.061 \\
    -8012.231 \\ 
    32849.055 \\ 
    18856.758 \\ 
    -8917.463 \\
    31289.131 \\ 
    568.08
\end{matrix}\right]$ & 
$\mbf{a}_2 = \left[\begin{matrix}
    137199.142 \\
    37238.975 \\
    -18303.661 \\
    6240.703 \\
    5099.608 \\
    -3584.203 \\
    28903.859 \\
    -3043.506 \\
    21158.897 \\
    -25565.128 \\
    -60256.757 \\
    34175.689 \\
    20013.206 \\
    -55.379 
\end{matrix}\right]$ & 
$\mbf{a}_3 = \left[\begin{matrix}
    90871.97 \\
    2481.881 \\
    -12760.566 \\
    -7305.273 \\
    6178.2 \\
    3980.622 \\
    -31851.585 \\
    -51828.543 \\
    -3346.438 \\
    -17933.398 \\
    39059.302 \\
    -20064.289 \\
    -62224.546 \\
    -595.573
\end{matrix}\right]$
\end{tabular}
\end{center}

This resulted in 78 correct classifications out of 89 (87.6\% accuracy).

\bigskip
\fakesection{Algorithm 5.8 (Batch Relaxation with Margin)}
Due to the large number of training iterations, training was capped to 100000 iterations.
\begin{center}
\begin{tabular}{|c|c|c|}
\multicolumn{3}{c}{{\bf Training Statistics}}\\
\hline{\bf Class} & {\bf Iterations} & {\bf Runtime (s)}\\\hline
$\omega_1$ & 100000 & 104.886 \\
$\omega_2$ & 100000 & 146.686 \\
$\omega_3$ & 100000 & 147.255\\\hline
{\bf Total} & 300000 & 398.827\\\hline
\end{tabular}
\end{center}

After training the perceptron using Algorithm 5.8, the weights were:
\begin{center}
\begin{tabular}{ ccc }
$\mbf{a}_1 = \left[\begin{matrix}
    0.582 \\
    -0.213 \\
    0.487 \\
    0.728 \\
    -0.381 \\
    -0.069 \\
    0.881 \\
    0.103 \\
    0.246 \\
    0.188 \\
    0.494 \\
    0.443 \\
    0.534 \\
    0.008
\end{matrix}\right]$ & 
$\mbf{a}_2 = \left[\begin{matrix}
    0.539 \\
    0.306 \\
    0.402 \\
    0.633 \\
    -0.193 \\
    -0.021 \\
    0.284 \\
    0.145 \\
    0.69  \\
    0.973 \\
    -0.288 \\
    0.054 \\
    0.657 \\
    -0.006
\end{matrix}\right]$ & 
$\mbf{a}_3 = \left[\begin{matrix}
    0.371 \\
    0.141 \\
    0.511 \\
    0.841 \\
    -0.176 \\
    -0.042 \\
    0.062 \\
    0.114 \\
    0.545 \\
    0.337 \\
    0.601 \\
    0.142 \\
    -0.14  \\
    -0.003
\end{matrix}\right]$
\end{tabular}
\end{center}

This resulted in 83 correct classifications out of 89 (93.26\% accuracy)

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\result{USPS Handwritten Digit Data Set}

\begin{center}
\fakesection{Algorithm 5.4 (Fixed-Increment Single-Sample Perceptron)}
\begin{tabular}{|c|c|c|}
\multicolumn{3}{c}{{\bf Training Statistics}}\\
\hline{\bf Class} & {\bf Iterations} & {\bf Runtime (s)}\\
\hline
$\omega_0$ & 15 & 0.061 \\
$\omega_1$ & 5 & 0.021 \\
$\omega_2$ & 13 & 0.049 \\
$\omega_3$ & 9 & 0.033 \\
$\omega_4$ & 24 & 0.081 \\
$\omega_5$ & 17 & 0.050 \\
$\omega_6$ & 11 & 0.033 \\
$\omega_7$ & 21 &  0.062 \\
$\omega_8$ & 20 & 0.059 \\
$\omega_9$ & 42 & 0.118 \\
\hline
{\bf Total} & 177 & 0.571\\
\hline
\end{tabular}
\end{center}

The weights are too large to display in the report, however they can be displayed by using the run.py script, by invoking the command ``python3 run.py fixed bin/digits\_train.txt bin/digits\_test.txt''

\bigskip

This resulted in 1598 correct classifications out of 2007 (79.62\% accuracy)

\bigskip
\fakesection{Algorithm 5.8 (Batch Relaxation with Margin)}
In order to reach convergence more quickly, the algorithm terminated when the change in weights was approximately zero, and $J_r(a) \approx 0$ (Equation (33) from Chapter 5).
\begin{center}
\begin{tabular}{|c|c|c|}
\multicolumn{3}{c}{{\bf Training Statistics}}\\
\hline{\bf Class} & {\bf Iterations} & {\bf Runtime (s)}\\
\hline
$\omega_0$ & 1114 & 2.923 \\
$\omega_1$ & 1424 & 3.833 \\
$\omega_2$ & 1106 & 2.955 \\
$\omega_3$ & 1179 & 3.351 \\
$\omega_4$ & 1357 & 3.856 \\
$\omega_5$ & 1524 & 4.176 \\
$\omega_6$ & 1331 & 3.617 \\
$\omega_7$ & 2335 & 6.104 \\
$\omega_8$ & 1189 & 3.186 \\
$\omega_9$ & 2561 & 7.022 \\
\hline
{\bf Total} & 15120 & 41.023\\
\hline
\end{tabular}
\end{center}

The weights are too large to display in the report, however they can be displayed by using the run.py script, by invoking the command ``python3 run.py relax bin/digits\_train.txt bin/digits\_test.txt''

\bigskip

This resulted in 1614 correct classifications out of 2007 (80.42\% accuracy)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Multi-Class Classification}
Use the basic two-class perceptron algorithms to solve multi-class classification problems by using the one-against-the-rest and one-against-the-other methods. Note that you need to handle ambiguous cases properly.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection*{Results}
{\large TODO}
For each dataset, now train a classifier to classify all the classes using the one-against-the-rest and the one-against-the-other methods based on the two two-class algorithms, resulting in four different classifiers on each dataset and then classify the test set. Document classification accuracy, iterations in training, and classification time for test, and compare the one-against-the- rest and the one-against-the-other methods.

\bigskip
\fakesection{UCI Wine Data Set}
\bigskip
\fakesection{USPS Handwritten Digits Data Set}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Adaboost to Create Strong Classifers}
Implement Algorithm 1 (AdaBoost) in Chapter 9 of the textbook to create a strong classifier using the above linear discriminant functions.

\begin{figure}[H]
\begin{algo}{Algorithm 9.1 (AdaBoost)}
\newcommand{\mbx}{\mbf{x}}
\State {\bf initialize} $\mc{D} = \mbrace{\mbx^1, y_1, ..., \mbx^n, y_n}, k_{max}, W_1(i) = 1/n, i = 1...n$
\State $k = 0$
\State {\bf do} ~$k \gets k+1$
\State\algtab train weak learner $C_k$ using \mc{D} sampled according to $W_k(i)$
\State\algtab $E_k \gets$ training error of $C_k$ measured on \mc{D} using $W_k(i)$
\State\algtab $\alpha_k \gets 0.5 \ln\left[ (1-E_k) / E_k \right]$
\State\algtab $W_{k+1}(i) = \frac{W_k(i)}{Z_k} \times \begin{cases}
e^{-\alpha_k} & \text{ if } h_x(\mbx^i) = y_i \text{ (correct classification)}\\
e^{\alpha_k} & \text{ if } h_k(\mbx^i) \neq y_i \text{ (incorrect classification)}
\end{cases}$
\State {\bf until} $k = k_{max}$
\State {\bf return} $C_k$ and $\alpha_k$ for $k = 1$ to $k_{max}$ (ensemble of classifiers with weights)
\end{algo}
\end{figure}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection*{Results}
Boost Algorithm 8 to create a strong classifier for class 1 vs. class 2, class 1 vs. class 3, and class 2 vs. class 3 on the two datasets. Then classify the corresponding test samples from the relevant classes in test sets (in other words, for example, for the class 1 vs. class 2 classifier, you only need to classify test samples from classes 1 and 2); then document classification accuracy and show and analyze the improvement.

\bigskip

\fakesection{UCI Wine Data Set}
\bigskip
\fakesection{USPS Handwritten Digits Data Set}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Extra Credit}
\subsection{Support vector machines}
By using an available quadratic programming optimizer or an SVM library, implement a training and classification algorithm for support vector machines. Then use your algorithm on the USPS dataset. Document the classification accuracy and compare the results with that from the two basic algorithms.

\subsubsection*{Results}
{\large TODO}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Kernel method for linear discriminant functions}
Given a kernel function, derive the kernel-version of Algorithm 4 and implement the algorithm, and then apply it on the given wine and USPS datasets. Document the classification accuracy and compare the results with that from the two basic algorithms without kernels. Use the polynomial function of degree three as the kernel function; optionally, you can use other commonly used kernel functions.

\subsubsection*{Results}
{\large TODO}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Multiple-class linear machines and multiple-class boosting}
Use the Kesler’s construction to train a linear machine for multi-class classification and then use the SAMME algorithm to boost its performance on the training set. Apply the algorithm on both datasets and classify the corresponding test samples in the test sets. Document the classification accuracy and compare the results with that from the one-against-the-rest and one-against-the- other algorithms.

\subsubsection{Results}
{\large TODO}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{9}

\bibitem{sp}
    Jones E, Oliphant E, Peterson P, \emph{et al.}
    {\bf SciPy: Open Source Scientific Tools for Python}, 2001-,
    \url{http://www.scipy.org/} [Online; accessed 2015-10-24].

\end{thebibliography}
\end{document}